# Security Emphasis Update â€” Senior Engineer Checklist

*Based on the pickle vulnerability catch in Sage v0.2*

---

## The Lesson

```python
# What we had (VULNERABLE)
np.load(embeddings_path, allow_pickle=True)

# What we shipped (SAFE)
np.load(embeddings_path, allow_pickle=False)  # .npy only
json.load(ids_path)                            # IDs separate
```

Pickle files can execute arbitrary code on load. This is a **Remote Code Execution (RCE)** vulnerability. If an attacker can control the pickle file, they own your system.

---

## Add to SENIOR_ENGINEER_CHECKLIST.md

### ðŸ”’ Security Engineer â€” Enhanced Deserialization Section

**The Serialization Attack Surface:**

```
NEVER trust these with untrusted data:
â”œâ”€â”€ pickle.load()           # Python RCE
â”œâ”€â”€ yaml.load()             # Use yaml.safe_load()
â”œâ”€â”€ eval() / exec()         # Obviously
â”œâ”€â”€ marshal.load()          # Python RCE
â”œâ”€â”€ subprocess with shell=True
â”œâ”€â”€ np.load(allow_pickle=True)
â”œâ”€â”€ torch.load()            # Uses pickle internally
â”œâ”€â”€ joblib.load()           # Uses pickle internally
â””â”€â”€ Any "load" that reconstructs objects

SAFE alternatives:
â”œâ”€â”€ JSON                    # Data only, no code
â”œâ”€â”€ numpy .npy              # Arrays only (allow_pickle=False)
â”œâ”€â”€ Protocol Buffers        # Schema-defined
â”œâ”€â”€ MessagePack             # Data only
â”œâ”€â”€ YAML (safe_load only)   # Subset of YAML
â””â”€â”€ SQLite / Postgres       # Structured data
```

**The Security Engineer's First Question:**

```
"What can an attacker control?"

If the answer includes ANY of:
â”œâ”€â”€ File contents on disk
â”œâ”€â”€ Network input
â”œâ”€â”€ Database values
â”œâ”€â”€ Environment variables
â”œâ”€â”€ User input (obviously)

Then: NEVER deserialize with pickle/eval/yaml.load
```

**Red Flags in Code Review:**

```python
# ðŸ”´ RED FLAG: pickle with external data
with open(user_provided_path, 'rb') as f:
    data = pickle.load(f)  # RCE if attacker controls file

# ðŸ”´ RED FLAG: torch.load on downloaded models
model = torch.load(downloaded_model_path)  # Pickle under the hood

# ðŸ”´ RED FLAG: yaml.load without safe_load
config = yaml.load(config_file)  # YAML can execute Python

# ðŸ”´ RED FLAG: numpy with pickle
embeddings = np.load(path, allow_pickle=True)  # RCE risk

# ðŸ”´ RED FLAG: joblib with user data
model = joblib.load(user_uploaded_model)  # Pickle under the hood
```

**Safe Patterns:**

```python
# âœ… SAFE: JSON for config/data
config = json.load(config_file)

# âœ… SAFE: yaml.safe_load
config = yaml.safe_load(config_file)

# âœ… SAFE: numpy without pickle
embeddings = np.load(path, allow_pickle=False)

# âœ… SAFE: torch with weights_only (PyTorch 2.0+)
model.load_state_dict(torch.load(path, weights_only=True))

# âœ… SAFE: Explicit schema validation
data = json.load(file)
validated = MySchema.model_validate(data)  # Pydantic
```

---

### Add to Security Engineer Checklist:

```
â–¡ Any pickle/torch/joblib loads?
  â””â”€â”€ Can attacker control the file? â†’ VULNERABLE

â–¡ Any yaml.load()?
  â””â”€â”€ Is it safe_load()? If not â†’ VULNERABLE

â–¡ Any np.load()?
  â””â”€â”€ Is allow_pickle=False? If not â†’ CHECK

â–¡ Any eval()/exec()?
  â””â”€â”€ Does input come from user? â†’ VULNERABLE

â–¡ Any subprocess calls?
  â””â”€â”€ Is shell=True with user input? â†’ VULNERABLE

â–¡ Model loading (ML projects)?
  â””â”€â”€ Using torch.load without weights_only? â†’ CHECK
  â””â”€â”€ Using joblib.load on user data? â†’ VULNERABLE
```

---

## Add to ENGINEERING_PRINCIPLES.md â€” Security Section

### Deserialization: The Silent Killer

```
The Principle:
"Never deserialize untrusted data into objects that can execute code."

The Test:
â”œâ”€â”€ Can an attacker control the serialized data?
â”œâ”€â”€ Does the deserializer execute code? (pickle, yaml, eval)
â””â”€â”€ If both yes â†’ you have RCE

The Fix:
â”œâ”€â”€ Use data-only formats (JSON, safe_load, protobuf)
â”œâ”€â”€ Validate schema after loading
â”œâ”€â”€ Sandbox if you MUST deserialize untrusted objects
```

### Dependency Awareness

```
Know what your dependencies do internally:

torch.load()     â†’ uses pickle
joblib.load()    â†’ uses pickle
np.load()        â†’ CAN use pickle (check allow_pickle)
yaml.load()      â†’ CAN execute Python
pandas.read_*()  â†’ Generally safe, but check pickle options
```

---

## Add to Smart Contract Checklist â€” SENIOR_SC_ENGINEER_CHECKLIST.md

**Note:** Smart contracts don't have pickle, but the principle applies:

```
Never trust external input to control code execution paths:

â–¡ Can calldata control which function executes? (proxy patterns)
â–¡ Can user input become part of a delegatecall target?
â–¡ Can user control the CREATE2 salt in ways that matter?
â–¡ Are there any assembly blocks that use user input as addresses?
```

---

## Quick Reference Card

| Library | Dangerous | Safe |
|---------|-----------|------|
| Python stdlib | `pickle.load()`, `eval()`, `exec()` | `json.load()` |
| PyYAML | `yaml.load()` | `yaml.safe_load()` |
| NumPy | `np.load(allow_pickle=True)` | `np.load(allow_pickle=False)` |
| PyTorch | `torch.load()` | `torch.load(weights_only=True)` |
| Joblib | `joblib.load()` on untrusted | N/A â€” don't use on untrusted |
| subprocess | `shell=True` + user input | `shell=False`, explicit args |

---

## The Meta-Principle

```
"If it can reconstruct arbitrary objects, it can execute arbitrary code."

Pickle is not a data format. It's a code format.
Treat it like you'd treat eval().
```

---

*Added after Sage v0.2 security review â€” the paranoia is a feature.*
